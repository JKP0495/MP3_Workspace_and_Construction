<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MP3 Workspace and Construction</title>
    <link rel="stylesheet" href="index.css" as="style">
</head>

<body style="padding: 0px;">
    <div id="__next">
        <div class=" w-full z-10 sticky bg-white top-0 border-b border-grey-light">
            <div class="w-full flex flex-wrap items-center lg:justify-between mt-0 py-4 justify-center">
                <div class=" mx-0 px-0 lg:pl-4 flex items-center lg:mx-4"><span
                        class="text-teal-700 no-underline hover:no-underline font-bold text-xl text-purple-800"><a
                            href="/">MP3 Workspace and Construction</a></span>
                </div>
            </div>
        </div>
        <div class="relative w-full hidden bg-white shadow-xl" id="search-content">
            <div class="container mx-auto py-4 text-black"><input id="searchfield" type="search" placeholder="Search..."
                    class="w-full text-grey-800 transition focus:outline-none focus:border-transparent p-2 appearance-none leading-normal text-xl lg:text-2xl">
            </div>
        </div>
        <div class="mx-auto my-1"></div>
        <div class=""
            style="position:fixed;top:0;left:0;height:2px;background:transparent;z-index:99999999999;width:100%">
            <div class="" style="height: 100%; background: purple; width: 0%; opacity: 1; color: purple;">
                <div
                    style="box-shadow: purple 0px 0px 10px, purple 0px 0px 10px; width: 5%; opacity: 1; position: absolute; height: 100%; transform: rotate(3deg) translate(0px, -4px); left: -5.5%;">
                </div>
            </div>
        </div>
        <div class="jsx-2880253090">
            <section class="jsx-2880253090 mx-2 justify-center align-middle">
                <div class="jsx-2880253090 flex flex-wrap lg:-mx-1 xl:-mx-1">
                    <div class="jsx-2880253090 w-full border lg:my-1 lg:px-1 lg:w-2/3 xl:my-1 xl:px-1 xl:w-2/3 mx-auto">
                        <div class="jsx-2880253090 h-[30vh] lg:h-[30vh] mx-auto xl:h-[70vh]"><iframe
                                title="YouTube Video" src="https://www.youtube.com/embed/DZKOunP-WLQ" frameborder="0"
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                                allowfullscreen="" class="jsx-2880253090 w-full h-full"></iframe></div>
                    </div>
                </div>
            </section>
            <!-- <nav class="jsx-2880253090 mx-2 px-3 tabs flex sm:flex-row overflow-x-scroll lg:overflow-x-hidden"><button
                    data-target="panel-1"
                    class="jsx-2880253090 tab active text-gray-600 py-2 px-2 block hover:text-blue-500 focus:outline-none border-b-2 font-medium border-blue-500">Overview</button><button
                    data-target="panel-2"
                    class="jsx-2880253090 tab active text-gray-600 py-2 px-2 block hover:text-blue-500 focus:outline-none  font-medium border-blue-500">Q&amp;A</button><button
                    data-target="panel-2"
                    class="jsx-2880253090 tab active text-gray-600 py-2 px-2 block hover:text-blue-500 focus:outline-none  font-medium border-blue-500">Downloads</button><button
                    data-target="panel-2"
                    class="jsx-2880253090 tab active text-gray-600 py-2 px-2 block hover:text-blue-500 focus:outline-none  font-medium border-blue-500">Announcements</button>
            </nav> -->
            <div id="panels" class="jsx-2880253090">
                <div class="jsx-2880253090 lg:w-2/3">
                    <h1 class="jsx-2880253090 text-2xl lg:text-3xl font-semibold mx-4 lg:mx-16 my-2">MP3 Workspace and Construction</h1>
                    <div class="jsx-2880253090 mx-4 lg:mx-16 panel-1 tab-content py-5 break-words">
                        <p>There is a particular irony to music activity. The entire online music industry is built upon a school subject that most of those young music fans would claim to dislike most: mathematics. What they are really downloading each night are streams of numbers, computed using a calculus-based technique first developed more than 200 years ago.</p>
                        <p>&nbsp;</p>
                        <p>

                            In the early 19th century, the French mathematician Joseph Fourier worked out some mathematical equations to describe the way heat disperses. Those same equations can be used to describe any waveform as a sequence of numbers, including sound waves. In the 60s, an American engineer called R A Moog used Fourier's math to design electronic music synthesizers.
                        </p>
                        <p>&nbsp;</p>
                        <p>
                            In the 80s, Yamaha, the Japanese electronics company, used the same maths to revolutionize the music industry with the introduction of electronic keyboard instruments. And today, Fourier's equations live on as part of the mathematical framework on which the music encoding scheme MP3 is based. A particularly fascinating aspect to MP3 is that it combines maths with knowledge of the way the human auditory system works. First, let's look at the maths.
                        </p>
                        <p>&nbsp;</p>
                        <p>
                            Fourier showed how any wave form (including a sound wave) can be broken down into constituent sine waves, those perfectly regular waves that engineers generate on oscilloscopes. The exact pattern of sine waves that combine to form a given sound wave can be represented by a sequence of numbers.</p>
                            <p>&nbsp;</p>
                            <p>
                            Those numbers may be computed from the original wave by a mathematical process known today as a Fourier Transform. In this way, sound may be turned into numbers. Actually, there is a bit more to it than that. The Fourier Transform works not on a wave but on a mathematical description of that wave. So first you have to convert a wave into a mathematical description.
                            </p>
                        <p>&nbsp;</p>
                        <p>The overall algorithm is broken up into 4 main parts.
                        </p>
                        <p>

                            ● Part 1 divides the audio signal into smaller pieces, these are called frames. An MDCT filter is then performed on the output.
                        </p>
                        <p>
                            ● Part 2 passes the sample into a 1024-point FFT, and then the psychoacoustic model is applied. Another MDCT filter is performed on the output.
                        </p>
                        <p>
                            ● Part 3 quantifies and encodes each sample. This is also known as noise allocation. The noise allocation adjusts itself in order to meet the bit rate and sound masking requirements.
                        </p>
                        <p>
                            ● Part 4 formats the bitstream, called an audio frame. An audio frame is made up of 4 parts, The Header, Error Check, Audio Data, and Ancillary Data.
                            </p>
                            <p>&nbsp;</p>
                    </div>
                    <h2 class="jsx-2880253090 text-2xl lg:text-3xl font-semibold mx-4 lg:mx-16 my-2">PART 1: MCDT (Modified Discrete Cosine Transform) FILTER</h2>
                    <div class="jsx-2880253090 mx-4 lg:mx-16 panel-1 tab-content py-5 break-words">
                        <p>The MDCT is a Fourier related transform based on type-IV DCT. It has an additional property of being “lapped.” This linear function transforms 2N real numbers to N real numbers according to the equation:
                            </p>
                            <p>
                            F:R2N→RN
                        </p>
                        <p>
                                        Xk=∑2N−1n=−∞xncosπN(n+12+N2)(k+12)
                                    </p>
                                    <p>&nbsp;</p>
                                    <p>
                       The MDCT limits the sources of output distortion at the quantization stage. It is also used as and analysis filter given by:
                    </p>
                    <p>
                       
                                        hk(n)=w(n)2M−−−√cos[(2n+M+1)(2k+1)(π)4M]
                                    </p>
                                    <p>&nbsp;</p>
                                    <p>
                       The MDCT performs a series of inner products between the input data x(n), and the analysis filter hk(n). This eliminates the blocking artifacts that would cause a problem during the reconstruction of the sample. The final signal is given by:
                    </p>
                    <p>
                                        x(n)=∑M−1k=0[X(k)hk(n)+XP(k)hk(n+M)]
                       </p>
                    </div>
                    <h2 class="jsx-2880253090 text-2xl lg:text-3xl font-semibold mx-4 lg:mx-16 my-2">PART 2: 1024-point FFT (Fast Fourier Transform)</h2>
                    <div class="jsx-2880253090 mx-4 lg:mx-16 panel-1 tab-content py-5 break-words">
                        <p>
                            The FFT is an algorithm that computes the Discrete Fourier Transform and its inverse. In general the DFT is found by using the equation:</p>
                            <p>


                 Xk=∑N−1n=0xne−2iπknN Where X0...XN−1are complex numbers and k = 0... N-1
                </p>
                <p>&nbsp;</p>
                <p>
The FFT is used as a filter bank on an audio sample. It is used to filter out unwanted or unneeded data from the sample.
</p>
<p>&nbsp;</p>
<p>
● First, incoming audio samples, s(n) , are normalized based the following equation x(n):
</p>
<p>
                 x(n)=s(n)N(2b−1)
                </p>
                <p>
Where N is the FFT length of the sample and b is the number of bits in the sample.
</p>
<p>&nbsp;</p>
<p>

● Second, the masking threshold of the sample is found by using an estimate of the power density spectrum, P(k). P(k) is computed by using a 1024-point FFT.
</p>
<p>
                 x(n)=Pk+10log[∑N−1n=0h(n)x(n)exp(−j2πknN)]2, 0 ≤ k ≤ N-1
                </p>
                <p>
Where h(n) is a Hann Window
</p>
<p>&nbsp;</p>
<p>
Engineers do this by taking note of a trick nature performed many years ago when she developed animal hearing systems. A sound wave consists of a ripple in the air.
</p>
<p>&nbsp;</p>
<p>
What makes it sound is that our ears and, more generally, our hearing system interpret that airwave as sound. The motion of the air causes a skin membrane in the inner ear to vibrate, and those vibrations are converted into tiny electrical currents that flow into the brain. It is those electrical waves that the brain actually experiences as sound.
</p>
<p>&nbsp;</p>
<p>
A microphone works in essentially the same way, converting an incoming sound wave in the air into an electrical signal. If we feed that electrical signal into a loudspeaker, then the speaker recreates (a copy of) the original sound wave. But we can also do something else to that electrical wave: we can use a method known as sampling to generate a sequence of numbers. The most common procedure is called Pulse Code Modulation (PCM).
</p>
<p>&nbsp;</p>
<p>


This takes an electrical wave and measures the voltage of the signal at moments of time a small, fixed interval apart. In the case of an audio compact disc, the sampling is done 44,100 times a second. Thus, for each second of sound input, the PCM analog-to-digital converter generates 44,100 numbers, each one the measurement of the voltage at the instant it is sampled.
</p>
<p>&nbsp;</p>
<p>
In the case of a compact disc, each voltage is measured to 16-bit accuracy; that is, the system can distinguish up to 65536 (or 2 to the power of 16) different voltages. A sample rate of 44,100 per second coupled with 16-bit voltage measurement is sufficient to encode any sound as a sequence of numbers that, when converted back into sound, the human ear cannot distinguish from the original.
Unfortunately, it takes a lot of storage capacity to capture music in this fashion: 10 megabytes for every minute of (stereo-recorded) music, to be precise.
</p>
<p>&nbsp;</p>
<p>
Given modern compact disc technology, this is fine for the recording industry, but would create a major problem if everyone were to ship CD music files around the internet. This is where Fourier's maths and a knowledge of the human hearing system comes in. Anyone with a computer is aware that there are algorithms that can compress data files (PK-ZIP and Stuffit are two well-known examples). When applied to a typical text file, these packages can reduce the size of the file by as much as 80%. But with CD quality PCM files, the reduction is only around 10%.
</p>
<p>&nbsp;</p>
<p>
Algorithms specially designed to operate on PCM files have achieved a 60% reduction, but that is nothing like enough to support internet music swapping. MP3 works by forgetting the idea of compressing the entire file so that the original sampled sound wave can be reproduced exactly. Instead, it deliberately discards some of the information. Thus, anything in the original sampled sound wave that the human hearing system cannot detect may be discarded.
</p>
<p>&nbsp;</p>
<p>
There is a lot of stuff that can be thrown away without our noticing. MP3 is short for MPeg3, or more fully MPEG - Level 3. MP3, an industry-standard developed in 1992 by the German Fraunhofer Research Institute, achieves a spectacular compression ratio of a sampled audio wave, ranging from a factor of eight to a factor of 12, depending on the source. This means that the 10MB of storage capacity needed to encode one minute of hi-fi music on a compact disc is reduced to a 1 MB MP3 file on a computer hard drive.
</p>
<p>&nbsp;</p>
<p>
MP3 divides the frequency range into 32 bands, each of which the human ear hears separately. The component of the input signal (sampled wave) in each of those ranges is then subjected to a Fourier-like mathematical transformation that separates it into a further 18 constituents, generating a total of 576 individual frequency bands. Within each of those bands, components undetectable to the human ear are removed.
</p>
<p>&nbsp;</p>
<p>
The resulting signal is then compressed further by Huffman coding, a technique familiar to computer scientists, which represents frequently occurring values by shorter codes than used for less frequently occurring values. (For instance, it would be highly wasteful to use the default 141,120 bits of the sampled wave to encode a 1/10 second silence in a song.) The result of all this maths?
</p>
<p>&nbsp;</p>
<p>
MP3 relies on two layers of compression, only one of which utilizes the psychoacoustic analysis. Psychoacoustic compression is best at reducing complicated sounds with lots of mixed components, because they provide plenty of masking opportunities. Simpler sounds do not benefit much from the psychoacoustic effects—but they can be easily compressed using more traditional data techniques. Combining both approaches requires a two-step process of quantization and Huffman coding, both of which feed back on each other to provide MP3 with its impressive bandwidth flexibility.
</p>
<p>&nbsp;</p>
<p>
The word "quantization" sounds complicated, but what it boils down to is the process of assigning a numerical value to something—giving it a quantity, in other words. "But we already have numbers," you might protest. "In fact, that's all we've been using all along!" This is true. But MP3 wouldn't be much of a compression method if it just converted some numbers wholesale from one form to another.
</p>
<p>&nbsp;</p>
<p>
Instead, the 576 post-MDCT frequency bins are sorted into 22 scalefactor bands. By dividing the values within the band by a given number (the quantizer) and rounding, a smaller approximation of the original is reached, but some information is lost during the rounding step. The quantization is the same across the entire frequency spectrum, but individual bands can be scaled up or down (hence the scalefactor) for more or less precision. Bigger numbers lose less information from the quantization process, while smaller numbers are more likely to suffer from rounding error. During the decoding, this scaling will be reversed so that the signal isn't any louder or softer.
</p>
<p>&nbsp;</p>
<p>
The job of all those Fast Fourier Transforms (FFTs) in the psychoacoustic model is to be able to specify how much precision is needed in a given scalefactor band. If there are weak signals that will be masked by stronger sounds in the band, the signal can be scaled down so that those signals are effectively truncated off. Unfortunately, the rounding errors also have audible effects (noise, basically), and the more a signal is reduced, the greater that noise will be when it is scaled back to its normal amplitude. So the second task of the psychoacoustic model is to check whether the signal-to-noise ratio has become perceptually unfavorable. If so, the encoder goes back to the quantization step and increases the scalefactor, increasing precision while reducing noise.
</p>
<p>&nbsp;</p>
<p>
This can be confusing, so let's use another hypothetical example. Let's say that our uncompressed scalefactor band information is represented by the number 12,592. We might quantize it by dividing by the number 100. So with a scalefactor of 1.0 (no change), we store this number as 126, and when we restore it during uncompression (multiplying it by the same quantization factor), we'll end up with 12,600. We sacrificed some precision and added a bit of noise—our number differs from the original by 8—but it's pretty close.
</p>
<p>&nbsp;</p>
<p>
However, if we were willing to put up with a little less precision and a little more noise, we could scale our original input by .1 to 1,259. Now it is quantized down to 13. In restoring the number, we apply both the quantization and undo the scale factor, ending up with a value of 13,000 (13 * 100 / .1 = 13,000). Now we're off by a bit more, but maybe not enough to notice, depending on the context. And those shorter storage values take up less room in a file, particularly when combined with the next stage in the process.

                        </p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</body>

</html>